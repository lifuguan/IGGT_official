<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>iGGT: Instance-Grounded Geometry Transformer</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #ffffff;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 0;
            box-shadow: none;
            overflow: hidden;
        }

        header {
            text-align: center;
            padding: 60px 40px 40px;
            background: #ffffff;
            color: #2d3748;
            border-bottom: 1px solid #e2e8f0;
        }

        .logo-container {
            margin-bottom: 25px;
        }

        h1 {
            font-size: 2.6em;
            margin-bottom: 10px;
            font-weight: 700;
            line-height: 1.3;
            max-width: 1000px;
            margin-left: auto;
            margin-right: auto;
            color: #1a202c;
        }

        h1 .project-name {
            font-size: 1.1em;
            font-weight: 800;
            color: #667eea;
        }

        .title-divider {
            display: inline;
            margin: 0 0.3em;
            color: #667eea;
        }

        .subtitle {
            font-size: 1.2em;
            margin-bottom: 25px;
            font-weight: 400;
            line-height: 1.5;
            color: #4a5568;
        }

        .conference-tag {
            display: inline-block;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 8px 20px;
            border-radius: 25px;
            font-size: 1em;
            font-weight: 600;
            margin: 15px 0 25px;
            box-shadow: 0 2px 8px rgba(102, 126, 234, 0.3);
        }

        .authors {
            font-size: 1.05em;
            margin: 25px auto;
            line-height: 1.8;
            max-width: 900px;
            color: #2d3748;
        }

        .authors a {
            color: #667eea;
            text-decoration: none;
            border-bottom: 1px solid rgba(102, 126, 234, 0.3);
            transition: all 0.3s ease;
        }

        .authors a:hover {
            border-bottom-color: #667eea;
            color: #5568d3;
        }

        .affiliations {
            font-size: 0.95em;
            margin: 20px auto;
            line-height: 1.6;
            max-width: 800px;
            color: #718096;
        }

        .keywords {
            display: flex;
            justify-content: center;
            gap: 10px;
            margin: 25px 0;
            flex-wrap: wrap;
        }

        .keyword-tag {
            background: #f7fafc;
            padding: 8px 18px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
            border: 1px solid #e2e8f0;
            color: #4a5568;
        }

        .buttons {
            display: flex;
            justify-content: center;
            gap: 12px;
            margin-top: 30px;
            flex-wrap: wrap;
        }

        .btn {
            display: inline-block;
            padding: 12px 28px;
            background: #667eea;
            color: white;
            text-decoration: none;
            border-radius: 50px;
            font-weight: 600;
            transition: all 0.3s ease;
            box-shadow: 0 2px 8px rgba(102, 126, 234, 0.3);
            font-size: 0.95em;
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
            background: #5568d3;
        }

        .content {
            padding: 60px 40px;
        }

        .section {
            margin-bottom: 60px;
        }

        .section-title {
            font-size: 2em;
            margin-bottom: 30px;
            text-align: center;
            color: #2d3748;
            font-weight: 600;
            position: relative;
            padding-bottom: 15px;
        }

        .section-title::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 50%;
            transform: translateX(-50%);
            width: 60px;
            height: 3px;
            background: #667eea;
            border-radius: 2px;
        }

        .demo-container {
            text-align: center;
            margin: 40px 0;
        }

        .demo-container img,
        .demo-container video {
            max-width: 100%;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
        }

        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }

        .feature-card {
            background: #f8f9fa;
            padding: 30px;
            border-radius: 15px;
            border: 1px solid #e2e8f0;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
            transition: all 0.3s ease;
        }

        .feature-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 4px 16px rgba(102, 126, 234, 0.15);
            border-color: #667eea;
        }

        .feature-card h3 {
            color: #667eea;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .feature-card p {
            color: #4a5568;
            line-height: 1.8;
        }

        .architecture-section {
            background: #f7fafc;
            padding: 40px;
            border-radius: 15px;
            margin: 40px 0;
            border: 1px solid #e2e8f0;
        }

        .architecture-section h3 {
            color: #667eea;
            margin: 25px 0 15px;
            font-size: 1.4em;
        }

        .architecture-section p,
        .architecture-section li {
            color: #4a5568;
            line-height: 1.8;
            margin-bottom: 15px;
        }

        .architecture-section ul {
            margin-left: 30px;
        }

        .formula {
            background: #ffffff;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            text-align: center;
            font-family: 'Courier New', monospace;
            border: 1px solid #e2e8f0;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
        }

        .image-caption {
            text-align: center;
            color: #718096;
            font-size: 0.95em;
            margin-top: 15px;
            font-style: italic;
        }

        .results-section {
            margin: 40px 0;
        }

        .citation {
            background: #f7fafc;
            color: #2d3748;
            padding: 30px;
            border-radius: 15px;
            margin: 40px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
            line-height: 1.6;
            position: relative;
            border: 1px solid #e2e8f0;
        }

        .citation::before {
            content: 'Citation';
            position: absolute;
            top: -12px;
            left: 20px;
            background: #667eea;
            color: white;
            padding: 5px 15px;
            border-radius: 5px;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
            font-size: 0.9em;
            box-shadow: 0 2px 4px rgba(102, 126, 234, 0.3);
        }

        .highlight-box {
            background: #f7fafc;
            border-left: 4px solid #667eea;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            border: 1px solid #e2e8f0;
            border-left: 4px solid #667eea;
        }

        footer {
            background: #f7fafc;
            color: #4a5568;
            text-align: center;
            padding: 30px;
            font-size: 0.95em;
            border-top: 1px solid #e2e8f0;
            margin-top: 60px;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.8em;
            }

            .subtitle {
                font-size: 1.1em;
            }

            .content {
                padding: 40px 20px;
            }

            .buttons {
                flex-direction: column;
                align-items: center;
            }

            .btn {
                width: 80%;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="logo-container">
                <img src="images/iggt_logo.png" alt="iGGT Logo" style="max-width: 200px; height: auto;">
            </div>
            <h1>
                <span class="project-name">IGGT</span><span class="title-divider">:</span> Instance-Grounded Geometry Transformer<br>
                for Semantic 3D Reconstruction
            </h1>
            
            
            <div class="authors">
                <a href="#">Hao Li</a><sup>1,2,3</sup>,
                <a href="#">Zhengyu Zou</a><sup>1</sup>,
                <a href="#">Fangfu Liu</a><sup>4</sup>,
                <a href="#">Xuanyang Zhang</a><sup>3*</sup>,
                <a href="#">Fangzhou Hong</a><sup>2</sup>,
                <br>
                <a href="#">Yukang Cao</a><sup>2</sup>,
                <a href="#">Yushi Lan</a><sup>2</sup>,
                <a href="#">Manyuan Zhang</a><sup>5</sup>,
                <a href="#">Gang Yu</a><sup>3</sup>,
                <a href="#">Dingwen Zhang</a><sup>1‚úâ</sup>,
                <a href="#">Ziwei Liu</a><sup>2</sup>
            </div>
            
            <div class="affiliations">
                <sup>1</sup>NWPU &nbsp;&nbsp;
                <sup>2</sup>S-Lab, NTU &nbsp;&nbsp;
                <sup>3</sup>StepFun, Inc. &nbsp;&nbsp;
                <sup>4</sup>THU &nbsp;&nbsp;
                <sup>5</sup>MMLab, CUHK
            </div>

            <div class="keywords">
                <span class="keyword-tag">Spatial Foundation Model</span>
                <span class="keyword-tag">Instance Spatial Tracking</span>
                <span class="keyword-tag">2D / 3D Open-Vocab. Segmentaion</span>
                <span class="keyword-tag">Scene Grounding</span>
            </div>
            
            <div class="buttons">
                <a href="https://huggingface.co/papers/2510.22706" class="btn" target="_blank">üìÑ Paper</a>
                <a href="https://arxiv.org/abs/2510.22706" class="btn" target="_blank">üìë arXiv</a>
                <a href="https://github.com/lifuguan/IGGT_official" class="btn" target="_blank">üíª Code</a>
                <a href="https://huggingface.co/datasets/lifuguan/InsScene-15K" class="btn" target="_blank">üóÇÔ∏è Dataset (Uploading)</a>
                <a href="#" class="btn" target="_blank">üìä Benchmark (Soon)</a>
            </div>
        </header>

        <div class="content">
            <!-- Demo Video Section -->
            <div class="section">
                <div class="demo-container">
                    <video autoplay loop muted playsinline style="max-width: 100%; border-radius: 15px; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);">
                        <source src="images/demo_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            </div>

            <!-- Introduction Section -->
            <div class="section">
                <h2 class="section-title">Revolutionary Solution</h2>
                <div class="highlight-box">
                    <p style="font-size: 1.1em; line-height: 1.8;">
                        Humans naturally perceive both geometric structure and semantic content of 3D worlds, but achieving "the best of both worlds" has been a grand challenge for AI. Traditional methods decouple 3D reconstruction (low-level geometry) from spatial understanding (high-level semantics), leading to error accumulation and poor generalization. Meanwhile, newer methods attempt to "lock" 3D models with specific Vision-Language Models (VLMs), which not only limits the model's perception capabilities (e.g., inability to distinguish between different instances of the same class) but also hinders extensibility to stronger downstream tasks.
                    </p>
                    <p style="font-size: 1.1em; line-height: 1.8; margin-top: 15px;">
                        Now, <strong>iGGT</strong> presents a revolutionary solution. NTU in collaboration with StepFun proposes <strong>iGGT (Instance-Grounded Geometry Transformer)</strong>, an innovative end-to-end large unified Transformer that, for the first time, integrates spatial reconstruction with instance-level contextual understanding.
                    </p>
                </div>
            </div>

            <!-- Teaser Image -->
            <div class="section">
                <div class="demo-container">
                    <img src="images/iggt_teaser.png" alt="iGGT Teaser">
                    <p class="image-caption">
                        building upon our curated large-scale dataset InsScene-15K, we propose a novel end-to-end framework that enables geometric reconstruction and contextual understanding in a unified representation.
                    </p>
                </div>
                
            </div>

            <!-- Key Contributions -->
            <div class="section">
                <h2 class="section-title">Key Contributions</h2>
                <div class="feature-grid">
                    <div class="feature-card">
                        <h3>üîß End-to-End Unified Framework</h3>
                        <p>We propose iGGT, a large unified Transformer that unifies knowledge of spatial reconstruction and instance-level contextual understanding through end-to-end training within a single model.</p>
                    </div>
                    <div class="feature-card">
                        <h3>üìä Large-Scale Instance Dataset</h3>
                        <p>We build InsScene-15K, a novel large-scale dataset containing 15K scenes, 200M images, and high-quality, 3D-consistent instance-level masks annotated through a novel data pipeline.</p>
                    </div>
                    <div class="feature-card">
                        <h3>üîå Instance Decoupling & Plug-and-Play</h3>
                        <p>We pioneer the "Instance-Grounded Scene Understanding" paradigm. iGGT is not bound to any specific VLM, but generates instance masks as a "bridge" for seamless plug-and-play integration with arbitrary VLMs and LMMs.</p>
                    </div>
                    <div class="feature-card">
                        <h3>üéØ Multi-Application Support</h3>
                        <p>This unified representation greatly expands downstream capabilities. iGGT is the first model to simultaneously support spatial tracking, open-vocabulary segmentation, and scene question answering (QA).</p>
                    </div>
                </div>
            </div>

            <!-- Dataset Section -->
            <div class="section">
                <h2 class="section-title">InsScene-15K Dataset Construction</h2>
                <div class="demo-container">
                    <img src="images/iggt_data_curation.png" alt="Data Curation Pipeline">
                    <p class="image-caption">
                        Overview of InsScene-15K dataset annotation pipeline.
                    </p>
                </div>

                <div class="architecture-section">
                    <p>
                        The InsScene-15K dataset is constructed through a novel data curation pipeline driven by SAM2, integrating three different data sources with distinct processing approaches:
                    </p>

                    <h3>üéÆ Synthetic Data (e.g., Aria, Infinigen)</h3>
                    <p>
                        This is the most straightforward case. In simulated environments, RGB images, depth maps, camera poses, and object-level segmentation masks are generated simultaneously. Since these simulated masks are "perfectly accurate," they can be used directly without any post-processing.
                    </p>

                    <h3>üé• Real-World Video Capture (e.g., RE10K)</h3>
                    <p>
                        This pipeline, illustrated in Figure 2(a), is a customized SAM2 video dense prediction pipeline. First, SAM generates dense initial mask proposals on frame 0 of the video. Then, the SAM2 video object segmenter propagates these masks forward in time. To handle newly appearing objects or avoid drift, the pipeline iteratively adds new keyframes: if uncovered regions exceed a threshold, SAM is re-run on new frames to discover new objects. Finally, bi-directional propagation is performed to ensure high temporal consistency throughout the entire video sequence.
                    </p>

                    <h3>üì∑ Real-World RGBD Capture (e.g., ScanNet++)</h3>
                    <p>
                        This pipeline, shown in Figure 2(b), is a mask refinement pipeline. The 3D annotations provided by ScanNet++ are coarse. The pipeline first projects these 3D annotations onto 2D images to obtain initial GT masks with consistent IDs. Meanwhile, SAM2 is used to generate fine-grained mask proposals with accurate shapes but without IDs for the same RGB image. The key step is matching and merging: aligning the fine masks generated by SAM2 with the projected coarse GT masks to assign correct, multi-view consistent IDs to the fine masks. Through this approach, the pipeline significantly improves the quality of 2D masks, maintaining both 3D ID consistency and SAM2-level shape accuracy.
                    </p>
                </div>
            </div>

            <!-- Architecture Section -->
            <div class="section">
                <h2 class="section-title">Architecture</h2>
                <div class="demo-container">
                    <img src="images/iggt_framework.png" alt="iGGT Framework">
                    <p class="image-caption">
                        Overview of iGGT architecture. Input images are encoded into unified token representations, which are then processed by the Geometry Head and Instance Head respectively to simultaneously generate high-quality geometric reconstruction and instance-grounded clustering results.
                    </p>
                </div>

                <div class="architecture-section">
                    <p>The iGGT architecture consists of three key components:</p>

                    <h3>üèóÔ∏è Large Unified Transformer</h3>
                    <p>
                        Following VGGT, the model first uses pretrained DINOv2 to extract patch-level tokens from images. Subsequently, 24 attention modules process the multi-view image tokens through <em>intra-view self-attention</em> and <em>global-view cross-attention</em>, encoding them into powerful unified token representations.
                    </p>

                    <h3>üéØ Downstream Heads and Cross-Modal Fusion</h3>
                    <p>The unified tokens are fed into two parallel decoders:</p>
                    <ul>
                        <li><strong>Geometry Head:</strong> Inherited from VGGT, responsible for predicting camera parameters, depth maps, and point clouds.</li>
                        <li><strong>Instance Head:</strong> Adopts a DPT-like architecture to decode instance features.</li>
                        <li><strong>Cross-Modal Fusion Block:</strong> To enable the instance head to perceive fine-grained geometric boundaries, we design a cross-modal fusion block. It efficiently embeds spatial structural features from the geometry head into instance representations through sliding window cross attention, significantly enhancing the spatial awareness of instance features.</li>
                    </ul>

                    <h3>üé® 3D-Consistent Contrastive Supervision</h3>
                    <p>
                        To enable the model to learn 3D-consistent instance features from only 2D inputs, we design a multi-view contrastive loss. The core idea is to "pull together" pixel features from different views belonging to the same 3D instance while "pushing apart" features from different instances in the feature space.
                    </p>
                </div>
            </div>

            <!-- Instance-Grounded Scene Understanding -->
            <div class="section">
                <h2 class="section-title">Instance-Grounded Scene Understanding</h2>
                <div class="architecture-section">
                    <p>
                        The core idea is to "decouple" the unified representation of the 3D model from downstream language models (VLMs or LMMs). This differs from previous methods that typically "tightly couple" or "forcibly align" 3D models with specific language models (like LSeg), which limits the model's perception capabilities and extensibility.
                    </p>
                    <p>
                        We first leverage unsupervised clustering (HDBSCAN) to group the 3D-consistent instance features predicted by iGGT, thereby segmenting the scene into different object instances. These clustering results are then reprojected to generate 3D-consistent 2D instance masks, which serve as a "bridge" for seamless plug-and-play integration with various VLMs (such as CLIP, OpenSeg) and LMMs (such as Qwen2.5-VL).
                    </p>
                    <p>This decoupling paradigm greatly expands the application scope of the model:</p>
                    <ul>
                        <li><strong>Instance Spatial Tracking:</strong> Using the 3D-consistent masks generated by clustering, we can densely track and segment specific object instances across multiple views, even under significant camera motion without easily losing the target.</li>
                        <li><strong>Open-Vocabulary Semantic Segmentation:</strong> Instance masks can serve as "prompts" fed into any off-the-shelf VLM (such as OpenSeg). The VLM assigns a semantic category to each masked region, enabling open-vocabulary segmentation.</li>
                        <li><strong>QA Scene Grounding:</strong> This decoupled instance clustering can interact with LMMs (such as GPT-4 or Qwen-VL 2.5). For example, we can highlight masks of the same instance across multiple views and then query the LMM to perform complex object-centric question answering tasks in 3D scenes.</li>
                    </ul>
                </div>
            </div>

            <!-- Results Section -->
            <div class="section">
                <h2 class="section-title">Experimental Results</h2>
                <div class="results-section">
                    <div class="highlight-box">
                        <p style="font-size: 1.1em; font-weight: 600;">
                            Compared to existing methods, iGGT is the only model capable of <strong>simultaneously achieving reconstruction, understanding, and tracking</strong>, with significant improvements in understanding and tracking metrics.
                        </p>
                    </div>
                    
                    <div class="demo-container" style="margin: 40px 0;">
                        <img src="images/ov_table_scannet.png" alt="ScanNet Results">
                    </div>

                    <div class="highlight-box">
                        <p style="font-size: 1.1em; font-weight: 600;">
                            On instance 3D tracking tasks, iGGT achieves tracking IoU and success rates of 70% and 90% respectively, making it <strong>the only model capable of successfully tracking objects that disappear and reappear</strong>.
                        </p>
                    </div>

                    <div class="demo-container" style="margin: 40px 0;">
                        <img src="images/spatial_track.png" alt="Spatial Tracking">
                        <p class="image-caption">
                            Our method is compared with SAM2 and SpaTracker+SAM. For clarity, all instances are visualized using different IDs and colors.
                        </p>
                    </div>

                    <div class="highlight-box">
                        <p style="font-size: 1.1em;">
                            We also conduct comprehensive visualization experiments on scenes, demonstrating that iGGT can <strong>generate 3D-consistent instance-based features</strong> that remain distinctive across multiple views: multiple instances of the same class exhibit similar yet distinguishable colors in PCA space.
                        </p>
                    </div>

                    <div class="demo-container" style="margin: 40px 0;">
                        <img src="images/pca_vis.png" alt="PCA Visualization">
                        <p class="image-caption">
                            We visualize 3D-consistent PCA results alongside instance feature-based clustering masks. Similar colors in PCA indicate higher feature similarity between instances. For clustering masks, the same object instance shares the same color across multiple views.
                        </p>
                    </div>

                    <div class="highlight-box" style="margin-top: 40px;">
                        <p style="font-size: 1.1em; font-weight: 600;">
                            On 2D/3D open-vocabulary segmentation tasks, thanks to the instance-grounded paradigm, we can <strong>seamlessly integrate</strong> the latest Vision-Language Models to enhance the model's query performance.
                        </p>
                    </div>

                    <div class="demo-container" style="margin: 40px 0;">
                        <img src="images/2d_ovs.png" alt="2D Open-Vocabulary Segmentation">
                        <p class="image-caption">
                            Qualitative results of 2D open-vocabulary segmentation on ScanNet and ScanNet++.
                        </p>
                    </div>

                    <div class="demo-container" style="margin: 40px 0;">
                        <img src="images/3d_ovs.png" alt="3D Open-Vocabulary Segmentation">
                        <p class="image-caption">
                            Qualitative results of 3D open-vocabulary segmentation on ScanNet and ScanNet++.
                        </p>
                    </div>

                    <div class="highlight-box">
                        <p style="font-size: 1.1em;">
                            Furthermore, we can leverage <strong>instance masks to construct visual prompts</strong> and integrate them with Large Multimodal Models (LMMs) such as Qwen-VL to enable <strong>more complex object-specific queries and question-answering tasks</strong> in scenes. In contrast, even state-of-the-art LMM models still have significant limitations in handling multi-view or 3D scene understanding.
                        </p>
                    </div>

                    <div class="demo-container" style="margin: 40px 0;">
                        <img src="images/scene_qa.png" alt="Scene QA">
                        <p class="image-caption">
                            Application of QA scene understanding compared with vanilla Gemini 2.5 Pro.
                        </p>
                    </div>
                </div>
            </div>

            <!-- Citation Section -->
            <div class="section">
                <h2 class="section-title">Citation</h2>
                <div class="citation">
@article{iggt2024,
  title={IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction},
  author={Hao Li and Zhengyu Zou and Fangfu Liu and Xuanyang Zhang and Fangzhou Hong and 
          Yukang Cao and Yushi Lan and Manyuan Zhang and Gang Yu and Dingwen Zhang and Ziwei Liu},
  journal={arXiv preprint arXiv:2510.22706},
  year={2024}
}
                </div>
            </div>
        </div>

        <footer>
            <p>¬© 2024 iGGT Project | NWPU ‚Ä¢ S-Lab, NTU ‚Ä¢ StepFun ‚Ä¢ THU ‚Ä¢ MMLab, CUHK</p>
            <p style="margin-top: 10px; opacity: 0.8;">A Revolutionary Instance-Understanding 3D Reconstruction Model</p>
        </footer>
    </div>
</body>
</html>
